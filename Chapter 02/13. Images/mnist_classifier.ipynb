{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classifier\n",
    "\n",
    "In this notebook you will create both, an mnist tabular dataset and a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- import the Operating System (os) module in python and any other library you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- As you can see each class has its own folder (Do it only for train). \n",
    "\n",
    "    - Iterate folder by folder ( os.listdir() )\n",
    "    - Inside each folder: \n",
    "        1.- Read the image\n",
    "        2.- Reshape it into a flat array (784,)\n",
    "        3.- Save the data into a pandas dataframe apending the column name as the class\n",
    "    - Save the data into a CSV\n",
    "\n",
    "    Note: if it takes to long try doing only 100 images per folder and the teacher for the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4132, 785)\n",
      "(4132, 785)\n",
      "(4684, 785)\n",
      "(4684, 785)\n",
      "(4177, 785)\n",
      "(4177, 785)\n",
      "(4351, 785)\n",
      "(4351, 785)\n",
      "(4072, 785)\n",
      "(4072, 785)\n",
      "(3795, 785)\n",
      "(3795, 785)\n",
      "(4137, 785)\n",
      "(4137, 785)\n",
      "(4401, 785)\n",
      "(4401, 785)\n",
      "(4063, 785)\n",
      "(4063, 785)\n",
      "(4188, 785)\n",
      "(4188, 785)\n"
     ]
    }
   ],
   "source": [
    "directory = 'C:/Users/andre/Documents/Strive_repository/local_exercise/Chapter 02/13. Images/archive/trainingSet/trainingSet/'\n",
    "\n",
    "# Determine the number of folder in the given directory\n",
    "dir_list = os.listdir(directory)\n",
    "\n",
    "# Initial DataFrame\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for file in dir_list:\n",
    "    images = os.listdir(directory + file)                           # from directory read images and files.\n",
    "    arr = np.zeros((len(images), 785))                              # built an array with 0 according to the dimensions of image and 785\n",
    "    print(arr.shape)  \n",
    "\n",
    "    for i, img in enumerate(images):                                # Return an enumerate object.\n",
    "        images_2 = Image.open(directory + file + '/' + img)         # Trying to open every single image with the for loop\n",
    "        arry = np.array(images_2, dtype=float)\n",
    "        # print(arry.shape)\n",
    "        arry = arry.flatten()                                       # Change the shape of the array to (784, )\n",
    "        # print(arry.shape)\n",
    "        arr[i, :784] = arry                                         # Read everything from the first column till the 783\n",
    "        arr[i, 784] = int(file)                                     # Read the column n. 784\n",
    "    \n",
    "    df = pd.DataFrame(data= arr)\n",
    "    df_conc = pd.concat([data, df])                                 # Merge everything in the first DataFrame 'data'\n",
    "    final_df = df_conc\n",
    "    print(final_df.shape)\n",
    "\n",
    "final_df.to_csv('data.csv', index= False, header=False)             # Saved the csv file\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Load the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7     8    9    ...  775  776  777  778  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  12.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   3.0  4.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   3.0  0.0  ...  4.0  5.0  2.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   779  780  781  782  783  784  \n",
      "0  0.0  0.0  0.0  0.0  0.0  9.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  9.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  9.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  9.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  9.0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "(4188, 784)\n",
      "(4188,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4188 entries, 0 to 4187\n",
      "Columns: 784 entries, 0 to 783\n",
      "dtypes: float64(784)\n",
      "memory usage: 25.1 MB\n",
      "----------------- \n",
      " None ----------------- \n",
      "\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 4188 entries, 0 to 4187\n",
      "Series name: 784\n",
      "Non-Null Count  Dtype  \n",
      "--------------  -----  \n",
      "4188 non-null   float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 32.8 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('C:/Users/andre/Documents/Strive_repository/local_exercise/Chapter 02/13. Images/data.csv', header=None)\n",
    "print(dataset.head())\n",
    "\n",
    "x = dataset.iloc[:, :-1]                    # Get all the rows and all the coloums apart from the last one (785)\n",
    "y = dataset.iloc[:, -1]                     # Get all the rows and the last coloum\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"----------------- \\n\" , x.info(), \"----------------- \\n\")\n",
    "print(y.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Create a dictionary of models (No preprocessing needed, it has already been done).\n",
    "    \n",
    "    Include both, tree models and mult models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "\n",
    "# Create a dictionary of models\n",
    "\n",
    "tree_classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100),\n",
    "    'Skl GBM': GradientBoostingClassifier(n_estimators=100),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'CatBoost': CatBoostClassifier(n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(n_estimators= 100),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100),\n",
    "    'SVM': SVC(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Using either cross validation or stratification find out which is the best model\n",
    "    - Base your code on the previous two days examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3350, 784)\n",
      "(838, 784)\n",
      "(3350,)\n",
      "(838,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, test_size= 0.2, stratify= y)\n",
    "\n",
    "# check the shape\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Can you rotate an image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50a93f2fecfd00da27f8930a2c1c74e92db967b2162384b3e8848f4306dc0d4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('deeplearner': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
